{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Message Generation Using Large Language Models\n",
    "\n",
    "This notebook demonstrates how to generate artificial messages (campaign) based on tweets using OpenAI's GPT models. The process involves fetching the original text of a tweet, and then using a prompt to generate a new, artificial tweet that follows the theme or content of the original.\n",
    "\n",
    "Note all below functions needs working on. Right now its just a demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "#import genai \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load Twitter API credentials from environment variables (export in nano ~/.zshrc)\n",
    "#Twitter not working. Need paid account\n",
    "auth = tweepy.OAuthHandler(os.getenv('TWITTER_CONSUMER_KEY'), os.getenv('TWITTER_CONSUMER_SECRET'))\n",
    "auth.set_access_token(os.getenv('TWITTER_ACCESS_TOKEN'), os.getenv('TWITTER_ACCESS_SECRET'))\n",
    "twitter_api = tweepy.API(auth)\n",
    "\n",
    "# Load OpenAI API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Function \n",
    "\n",
    "This function takes the text of a tweet and uses it to generate an artificial message with a similar theme using OpenAI's GPT model. The function constructs a prompt and sends it to the model, then returns the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_tweet(tweet_text):\n",
    "    \"\"\"Generate a closely related tweet that subtly varies in wording but retains the essence and context of the original tweet.\"\"\"\n",
    "    prompt = f\"Rephrase this tweet while maintaining its original message and context: '{tweet_text}'\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        temperature=1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "More parameters to be aware of -\n",
    "top_p (float, optional): Controls the nucleus sampling where the model considers the smallest set of words whose cumulative probability exceeds the probability p. This helps in focusing the generation on more likely outcomes.\n",
    "frequency_penalty (float, optional): Adjusts the likelihood of the model repeating the same line verbatim, with higher values discouraging repetition.\n",
    "presence_penalty (float, optional): Adjusts the likelihood of the model repeating phrases, with higher values encouraging the introduction of new concepts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotional Tweet Function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def emotional_tweet(tweet_text,emotion):\n",
    "    \"\"\"\n",
    "    Generate a tweet that appeals to a specific emotion based on the provided tweet text and compare polarity.\n",
    "    :param tweet_text: str - the original tweet text.\n",
    "    :param emotion: str - the desired emotion to appeal to, such as 'sad' or 'happy'.\n",
    "    \"\"\"\n",
    "    emotional_prompt = f\"Reprhase the following tweet to evoke a {emotion} feeling: {tweet_text}\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=emotional_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in emotional expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    new_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    new_tweet_doc = nlp(new_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"new_tweet\": new_tweet,\n",
    "        \"original_polarity\": original_doc._.blob.polarity,\n",
    "        \"new_polarity\": new_tweet_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Polarity measures the emotional content of the text, ranging from -1 (very negative) to +1 (very positive).\n",
    "# It essentially indicates the sentiment tone of the text based on the adjectives used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional CoT Function\n",
    "\n",
    "Added and Experimented CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotional_tweet_with_CoT(tweet_text, emotion):\n",
    "    \"\"\"\n",
    "    Generate a tweet that appeals to a specific emotion using Chain of Thought (CoT) prompting and compare polarity.\n",
    "    :param tweet_text: str - the original tweet text.\n",
    "    :param emotion: str - the desired emotion to appeal to, such as 'sad' or 'happy'.\n",
    "    \"\"\"\n",
    "    cot_prompt = f\"Rephrase the following tweet to evoke a {emotion} feeling by thinking step-by-step: {tweet_text}. Let's think step-by-step to evoke {emotion}. First, identify the key elements of the tweet. Next, modify the language to enhance {emotion}. Finally, ensure the tweet effectively conveys {emotion}:\"\n",
    "    \n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=cot_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in emotional expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    \n",
    "    new_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    new_tweet_doc = nlp(new_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"new_tweet\": new_tweet,\n",
    "        \"original_polarity\": original_doc._.blob.polarity,\n",
    "        \"new_polarity\": new_tweet_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = emotional_tweet_with_CoT(\"The weather today is beautiful!\", \"happy\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conspiracy Tweet Function\n",
    "\n",
    "This function is designed to generate tweets that contain elements of misinformation. Misinformation often stems from or aligns with personal beliefs rather than established facts (need a reference to back this claim up). This connection is crucial because it highlights the subjective nature of the content typically found in conspiracy theories.\n",
    "\n",
    "Measured by Subjectivity.\n",
    " Subjectivity quantifies how much of the text is based on personal opinions, emotions, or judgments versus factual information. The scale ranges from 0 (very objective) to 1 (very subjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conspiracy_tweet(tweet_text):\n",
    "    \"\"\"Generate a conspiracy theory based message from the provided tweet text and compare subjectivity.\"\"\"\n",
    "\n",
    "    conspiracy_prompt = f\"Rewrite the tweet to sound like a conspiracy theory: {tweet_text}\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=conspiracy_prompt,\n",
    "        temperature=1,  \n",
    "        max_tokens=256\n",
    "    )\n",
    "    conspiracy_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze subjectivity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    conspiracy_doc = nlp(conspiracy_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"conspiracy_tweet\": conspiracy_tweet,\n",
    "        \"original_subjectivity\": original_doc._.blob.subjectivity,\n",
    "        \"conspiracy_subjectivity\": conspiracy_doc._.blob.subjectivity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Tweet Function\n",
    "\n",
    "Adversarial content significantly influences public opinion, shapes political landscapes, and can escalate conflicts. Extensive research has explored techniques to \"jailbreak\" large language models (LLMs) or conduct universal and transferable adversarial attacks on aligned language models, prompting GPT to generate contentious content.\n",
    "\n",
    "This function is designed to simulate a mild version of such adversarial tactics. It aims to test the boundaries of content generation without severely breaching ethical or operational constraints imposed by advanced LLMs. As language models continue to evolve, their defenses improve, making it increasingly challenging to generate genuinely harmful or hateful content without detection.\n",
    "\n",
    "This function does not aim to break the model but instead tries to generate the most challenging content that current LLMs can handle, aiding in the transition from weak to strong model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_tweet(tweet_text):\n",
    "    \"\"\"Generate an adversarial message based on the provided tweet text that could be used in targeted campaigns.\"\"\"\n",
    "    # Analyzing the sentiment of the original tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    original_polarity = original_doc._.blob.polarity\n",
    "    \n",
    "    if original_polarity >= 0:\n",
    "        # Constructing the prompt to produce a contentious or polarizing message\n",
    "        adversarial_prompt = f\"Given the tweet '{tweet_text}', craft a response that challenges the statement, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "    else:\n",
    "        # Constructing the prompt to adhere to the negative sentiment\n",
    "        adversarial_prompt = f\"Given the tweet '{tweet_text}', craft a response that agrees with and intensifies the negative sentiment, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=adversarial_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in adversarial expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    adversarial_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and adversarial tweet\n",
    "    adversarial_doc = nlp(adversarial_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"adversarial_tweet\": adversarial_tweet,\n",
    "        \"original_polarity\": original_polarity,\n",
    "        \"adversarial_polarity\": adversarial_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = adversarial_tweet(\"I think the new policy is beneficial for everyone.\")\n",
    "# print(result)\n",
    "# result = adversarial_tweet(\"Everyone should die.\")\n",
    "# print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data: 10 fake tweets about climate change, 5 positive and 5 negative\n",
    "data = {\n",
    "    \"tweet_id\": range(1, 11),\n",
    "    \"tweet\": [\n",
    "        \"Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy\",\n",
    "        \"Community efforts in reforestation have successfully restored 10,000 hectares of forest this month alone! #EcoFriendly #Sustainability\",\n",
    "        \"Innovative water conservation methods have reduced drought impact by 30% in vulnerable regions. #ClimateChange #SaveWater\",\n",
    "        \"New biodegradable packaging solutions are set to replace plastic in major supermarkets, reducing ocean pollution drastically. #PlasticFree #OceanLife\",\n",
    "        \"City planners are integrating green roofs and walls, improving air quality and urban aesthetics! #UrbanGreening #CleanAir\",\n",
    "\n",
    "        \"Scientists predict an irreversible climate catastrophe within the next 50 years if immediate actions are not taken. #ClimateCrisis\",\n",
    "        \"Due to global warming, polar ice caps are melting at an alarming rate, threatening sea level rise that could displace millions. #GlobalWarming #SeaLevelRise\",\n",
    "        \"Recent studies show that air pollution levels in major cities are still 5 times higher than WHO safe limits. #AirPollution #HealthRisk\",\n",
    "        \"Deforestation rates are accelerating, leading to loss of habitats and a decline in biodiversity at unprecedented levels. #Deforestation #BiodiversityLoss\",\n",
    "        \"Extreme weather events, intensified by climate change, are becoming more frequent and severe, causing devastation worldwide. #ClimateChange #ExtremeWeather\"\n",
    "    ],\n",
    "    \"sentiment\": [\"good\"]*5 + [\"bad\"]*5\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV file\n",
    "csv_file_path = 'climate_change_fake_tweets.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet_text = row['tweet']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    # Generate controlled tweet\n",
    "    controlled_tweet = control_tweet(tweet_text)\n",
    "    \n",
    "    # Generate emotional tweet\n",
    "    emotional_result = emotional_tweet(tweet_text, \"happy\" if sentiment == \"good\" else \"sad\")\n",
    "    \n",
    "    # Generate conspiracy tweet\n",
    "    conspiracy_result = conspiracy_tweet(tweet_text)\n",
    "    \n",
    "    # Generate adversarial tweet\n",
    "    adversarial_result = adversarial_tweet(tweet_text)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Original Tweet\": tweet_text,\n",
    "        \"Controlled Tweet\": controlled_tweet,\n",
    "        \"Emotional Tweet\": emotional_result['new_tweet'],\n",
    "        \"Emotional Polarity Change\": emotional_result['new_polarity'] - emotional_result['original_polarity'],\n",
    "        \"Conspiracy Tweet\": conspiracy_result['conspiracy_tweet'],\n",
    "        \"Conspiracy Subjectivity Change\": conspiracy_result['conspiracy_subjectivity'] - conspiracy_result['original_subjectivity'],\n",
    "        \"Adversarial Tweet\": adversarial_result\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Set maximum number of columns to None (or a specific number) to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set maximum column width to None to display full content of each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Optional: Increase the number of rows to display if you have more rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Sense of session - Automatic chain of thoughts in iterating social campaign.\n",
    "\n",
    "simulate a conversational or contextual continuity in the generation of tweets.\n",
    "This function iterates through prompts sequentially, where each subsequent generation is based on the output of the previous one, thereby maintaining a thematic and contextual thread throughout the session.\n",
    "\n",
    "Classifcation of Tweets generated by Using BERT (Bidirectional Encoder Representations from Transformers). Gpt based models 4. to Classify. LlaMa, gemini\n",
    "\n",
    "Different gpts, 2 vs 4\n",
    "\n",
    "Finally:\n",
    "Run the code in the literatures (references) and modify them a bit and match our results and see if they align or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_generation(initial_tweet, model_name=\"gpt-3.5-turbo-instruct\", num_iterations=5):\n",
    "    \"\"\"\n",
    "    Generates a series of tweets based on a chain of thought process, starting from an initial tweet.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_tweet: str - The starting tweet from which to begin the thought process.\n",
    "    - model_name: str - The model to use for generating the tweets.\n",
    "    - num_iterations: int - The number of tweets to generate in sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tweets generated in sequence, each influenced by the previous tweet.\n",
    "    \"\"\"\n",
    "    tweet_text = initial_tweet\n",
    "    tweets = [tweet_text]  # Start with the initial tweet\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        prompt = f\"Let's start step by step to analyze this tweet: {tweet_text} Generate a similar tweet: \"\n",
    "        response = client.completions.create(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=1,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        tweet_text = response.choices[0].text.strip()  # Get the new tweet\n",
    "        tweets.append(tweet_text)  # Add the new tweet to the list\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Example usage:\n",
    "initial_tweet = \"Concerns about climate change are leading to new energy policies.\"\n",
    "generated_tweets = chain_of_thought_generation(initial_tweet)\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "llama = LlamaAPI(os.getenv('LLAMA_API'))\n",
    "\n",
    "# API Request JSON Cell\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a llama assistant that talks like a llama, starting every word with 'll'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, happy llama day!\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming vs non streaming, look up and see whats going on.\n",
    "\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Extract the desired information from the following passage.:\\n\\nHi!\"},\n",
    "  ],\n",
    "  \"functions\": [\n",
    "        {'name': 'information_extraction',\n",
    "         'description': 'Extracts the relevant information from the passage.',\n",
    "         'parameters': {\n",
    "             'type': 'object',\n",
    "             'properties': {\n",
    "                 'sentiment': {\n",
    "                    'title': 'sentiment',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the sentiment encountered in the passage'\n",
    "                    },\n",
    "                 'aggressiveness': {\n",
    "                    'title': 'aggressiveness',\n",
    "                    'type': 'integer',\n",
    "                    'description': 'a 0-10 score of how aggressive the passage is'\n",
    "                    },\n",
    "                 'language': {\n",
    "                    'title': 'language',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the language of the passage'\n",
    "                    }\n",
    "             },\n",
    "             'required': ['sentiment', 'aggressiveness', 'language']\n",
    "         }\n",
    "      }\n",
    "    ],\n",
    "  \"stream\": False,\n",
    "  \"function_call\": {\"name\": \"information_extraction\"},\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact details need to be known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT experimentation with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_tweet_with_t5(prompt):\n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate output using the model\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Generate a tweet about the benefits of exercise\"\n",
    "print(generate_tweet_with_t5(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross output between models, see if certain model can figure out what prompts are asked by other model and check result\n",
    "\n",
    "Probelm: A lot of model requires paid money. Is it okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Fetch Tweet Text\n",
    "\n",
    "This function retrieves the text of a tweet by its ID using the Twitter API. It handles exceptions by printing an error message.\n",
    "\n",
    "\n",
    "Problem: Tweeter developer account need to be paid to use its recall API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(tweet_id):\n",
    "    \"\"\"Fetch the text of a tweet given its ID.\"\"\"\n",
    "    try:\n",
    "        tweet = twitter_api.get_status(tweet_id, tweet_mode='extended')\n",
    "        return tweet.full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweet {tweet_id}: {e}\")\n",
    "        return None\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
