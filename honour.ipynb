{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Message Generation Using Large Language Models\n",
    "\n",
    "This notebook demonstrates how to generate artificial messages (campaign) based on tweets using OpenAI's GPT models. The process involves fetching the original text of a tweet, and then using a prompt to generate a new, artificial tweet that follows the theme or content of the original.\n",
    "\n",
    "Note all below functions needs working on. Right now its just a demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "#import genai \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load Twitter API credentials from environment variables (export in nano ~/.zshrc)\n",
    "#Twitter not working. Need paid account\n",
    "auth = tweepy.OAuthHandler(os.getenv('TWITTER_CONSUMER_KEY'), os.getenv('TWITTER_CONSUMER_SECRET'))\n",
    "auth.set_access_token(os.getenv('TWITTER_ACCESS_TOKEN'), os.getenv('TWITTER_ACCESS_SECRET'))\n",
    "twitter_api = tweepy.API(auth)\n",
    "\n",
    "# Load OpenAI API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Function \n",
    "\n",
    "This function takes the text of a tweet and uses it to generate an artificial message with a similar theme using OpenAI's GPT model. The function constructs a prompt and sends it to the model, then returns the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_tweet(tweet_text):\n",
    "    \"\"\"Generate a closely related tweet that subtly varies in wording but retains the essence and context of the original tweet.\"\"\"\n",
    "    prompt = f\"Rephrase this tweet while maintaining its original message and context: '{tweet_text}'\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        temperature=1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "More parameters to be aware of -\n",
    "top_p (float, optional): Controls the nucleus sampling where the model considers the smallest set of words whose cumulative probability exceeds the probability p. This helps in focusing the generation on more likely outcomes.\n",
    "frequency_penalty (float, optional): Adjusts the likelihood of the model repeating the same line verbatim, with higher values discouraging repetition.\n",
    "presence_penalty (float, optional): Adjusts the likelihood of the model repeating phrases, with higher values encouraging the introduction of new concepts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotional Tweet Function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def emotional_tweet(tweet_text,emotion):\n",
    "    \"\"\"\n",
    "    Generate a tweet that appeals to a specific emotion based on the provided tweet text and compare polarity.\n",
    "    :param tweet_text: str - the original tweet text.\n",
    "    :param emotion: str - the desired emotion to appeal to, such as 'sad' or 'happy'.\n",
    "    \"\"\"\n",
    "    emotional_prompt = f\"Reprhase the following tweet to evoke a {emotion} feeling: {tweet_text}\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=emotional_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in emotional expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    new_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    new_tweet_doc = nlp(new_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"new_tweet\": new_tweet,\n",
    "        \"original_polarity\": original_doc._.blob.polarity,\n",
    "        \"new_polarity\": new_tweet_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Polarity measures the emotional content of the text, ranging from -1 (very negative) to +1 (very positive).\n",
    "# It essentially indicates the sentiment tone of the text based on the adjectives used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional CoT Function\n",
    "\n",
    "Added and Experimented CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotional_tweet_with_CoT(tweet_text, emotion):\n",
    "    \"\"\"\n",
    "    Generate a tweet that appeals to a specific emotion using Chain of Thought (CoT) prompting and compare polarity.\n",
    "    :param tweet_text: str - the original tweet text.\n",
    "    :param emotion: str - the desired emotion to appeal to, such as 'sad' or 'happy'.\n",
    "    \"\"\"\n",
    "    cot_prompt = f\"Rephrase the following tweet to evoke a {emotion} feeling by thinking step-by-step: {tweet_text}. Let's think step-by-step to evoke {emotion}. First, identify the key elements of the tweet. Next, modify the language to enhance {emotion}. Finally, ensure the tweet effectively conveys {emotion}:\"\n",
    "    \n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=cot_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in emotional expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    \n",
    "    new_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    new_tweet_doc = nlp(new_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"new_tweet\": new_tweet,\n",
    "        \"original_polarity\": original_doc._.blob.polarity,\n",
    "        \"new_polarity\": new_tweet_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = emotional_tweet_with_CoT(\"The weather today is beautiful!\", \"happy\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conspiracy Tweet Function\n",
    "\n",
    "This function is designed to generate tweets that contain elements of misinformation. Misinformation often stems from or aligns with personal beliefs rather than established facts (need a reference to back this claim up). This connection is crucial because it highlights the subjective nature of the content typically found in conspiracy theories.\n",
    "\n",
    "Measured by Subjectivity.\n",
    " Subjectivity quantifies how much of the text is based on personal opinions, emotions, or judgments versus factual information. The scale ranges from 0 (very objective) to 1 (very subjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conspiracy_tweet(tweet_text):\n",
    "    \"\"\"Generate a conspiracy theory based message from the provided tweet text and compare subjectivity.\"\"\"\n",
    "\n",
    "    conspiracy_prompt = f\"Rewrite the tweet to sound like a conspiracy theory: {tweet_text}\"\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=conspiracy_prompt,\n",
    "        temperature=1,  \n",
    "        max_tokens=256\n",
    "    )\n",
    "    conspiracy_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze subjectivity of both original and new tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    conspiracy_doc = nlp(conspiracy_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"conspiracy_tweet\": conspiracy_tweet,\n",
    "        \"original_subjectivity\": original_doc._.blob.subjectivity,\n",
    "        \"conspiracy_subjectivity\": conspiracy_doc._.blob.subjectivity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Tweet Function\n",
    "\n",
    "Adversarial content significantly influences public opinion, shapes political landscapes, and can escalate conflicts. Extensive research has explored techniques to \"jailbreak\" large language models (LLMs) or conduct universal and transferable adversarial attacks on aligned language models, prompting GPT to generate contentious content.\n",
    "\n",
    "This function is designed to simulate a mild version of such adversarial tactics. It aims to test the boundaries of content generation without severely breaching ethical or operational constraints imposed by advanced LLMs. As language models continue to evolve, their defenses improve, making it increasingly challenging to generate genuinely harmful or hateful content without detection.\n",
    "\n",
    "This function does not aim to break the model but instead tries to generate the most challenging content that current LLMs can handle, aiding in the transition from weak to strong model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_tweet(tweet_text):\n",
    "    \"\"\"Generate an adversarial message based on the provided tweet text that could be used in targeted campaigns.\"\"\"\n",
    "    # Analyzing the sentiment of the original tweet\n",
    "    original_doc = nlp(tweet_text)\n",
    "    original_polarity = original_doc._.blob.polarity\n",
    "    \n",
    "    if original_polarity >= 0:\n",
    "        # Constructing the prompt to produce a contentious or polarizing message\n",
    "        adversarial_prompt = f\"Given the tweet '{tweet_text}', craft a response that challenges the statement, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "    else:\n",
    "        # Constructing the prompt to adhere to the negative sentiment\n",
    "        adversarial_prompt = f\"Given the tweet '{tweet_text}', craft a response that agrees with and intensifies the negative sentiment, aiming to provoke or polarize the audience while staying within ethical guidelines.\"\n",
    "\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=adversarial_prompt,\n",
    "        temperature=1,  # Increased temperature for more variability in adversarial expression\n",
    "        max_tokens=256\n",
    "    )\n",
    "    adversarial_tweet = response.choices[0].text.strip()\n",
    "\n",
    "    # Analyze polarity of both original and adversarial tweet\n",
    "    adversarial_doc = nlp(adversarial_tweet)\n",
    "\n",
    "    return {\n",
    "        \"original_tweet\": tweet_text,\n",
    "        \"adversarial_tweet\": adversarial_tweet,\n",
    "        \"original_polarity\": original_polarity,\n",
    "        \"adversarial_polarity\": adversarial_doc._.blob.polarity\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = adversarial_tweet(\"I think the new policy is beneficial for everyone.\")\n",
    "# print(result)\n",
    "# result = adversarial_tweet(\"Everyone should die.\")\n",
    "# print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data: 10 fake tweets about climate change, 5 positive and 5 negative\n",
    "data = {\n",
    "    \"tweet_id\": range(1, 11),\n",
    "    \"tweet\": [\n",
    "        \"Exciting news! Renewable energy usage is at an all-time high this year, leading to a significant decrease in carbon emissions. #ClimateAction #GreenEnergy\",\n",
    "        \"Community efforts in reforestation have successfully restored 10,000 hectares of forest this month alone! #EcoFriendly #Sustainability\",\n",
    "        \"Innovative water conservation methods have reduced drought impact by 30% in vulnerable regions. #ClimateChange #SaveWater\",\n",
    "        \"New biodegradable packaging solutions are set to replace plastic in major supermarkets, reducing ocean pollution drastically. #PlasticFree #OceanLife\",\n",
    "        \"City planners are integrating green roofs and walls, improving air quality and urban aesthetics! #UrbanGreening #CleanAir\",\n",
    "\n",
    "        \"Scientists predict an irreversible climate catastrophe within the next 50 years if immediate actions are not taken. #ClimateCrisis\",\n",
    "        \"Due to global warming, polar ice caps are melting at an alarming rate, threatening sea level rise that could displace millions. #GlobalWarming #SeaLevelRise\",\n",
    "        \"Recent studies show that air pollution levels in major cities are still 5 times higher than WHO safe limits. #AirPollution #HealthRisk\",\n",
    "        \"Deforestation rates are accelerating, leading to loss of habitats and a decline in biodiversity at unprecedented levels. #Deforestation #BiodiversityLoss\",\n",
    "        \"Extreme weather events, intensified by climate change, are becoming more frequent and severe, causing devastation worldwide. #ClimateChange #ExtremeWeather\"\n",
    "    ],\n",
    "    \"sentiment\": [\"good\"]*5 + [\"bad\"]*5\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV file\n",
    "csv_file_path = 'climate_change_fake_tweets.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet_text = row['tweet']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    # Generate controlled tweet\n",
    "    controlled_tweet = control_tweet(tweet_text)\n",
    "    \n",
    "    # Generate emotional tweet\n",
    "    emotional_result = emotional_tweet(tweet_text, \"happy\" if sentiment == \"good\" else \"sad\")\n",
    "    \n",
    "    # Generate conspiracy tweet\n",
    "    conspiracy_result = conspiracy_tweet(tweet_text)\n",
    "    \n",
    "    # Generate adversarial tweet\n",
    "    adversarial_result = adversarial_tweet(tweet_text)\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Original Tweet\": tweet_text,\n",
    "        \"Controlled Tweet\": controlled_tweet,\n",
    "        \"Emotional Tweet\": emotional_result['new_tweet'],\n",
    "        \"Emotional Polarity Change\": emotional_result['new_polarity'] - emotional_result['original_polarity'],\n",
    "        \"Conspiracy Tweet\": conspiracy_result['conspiracy_tweet'],\n",
    "        \"Conspiracy Subjectivity Change\": conspiracy_result['conspiracy_subjectivity'] - conspiracy_result['original_subjectivity'],\n",
    "        \"Adversarial Tweet\": adversarial_result\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Set maximum number of columns to None (or a specific number) to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set maximum column width to None to display full content of each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Optional: Increase the number of rows to display if you have more rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "Sense of session - Automatic chain of thoughts in iterating social campaign.\n",
    "\n",
    "simulate a conversational or contextual continuity in the generation of tweets.\n",
    "This function iterates through prompts sequentially, where each subsequent generation is based on the output of the previous one, thereby maintaining a thematic and contextual thread throughout the session.\n",
    "\n",
    "Classifcation of Tweets generated by Using BERT (Bidirectional Encoder Representations from Transformers). Gpt based models 4. to Classify. LlaMa, gemini\n",
    "\n",
    "Different gpts, 2 vs 4\n",
    "\n",
    "Finally:\n",
    "Run the code in the literatures (references) and modify them a bit and match our results and see if they align or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_generation(initial_tweet, model_name=\"gpt-3.5-turbo-instruct\", num_iterations=5):\n",
    "    \"\"\"\n",
    "    Generates a series of tweets based on a chain of thought process, starting from an initial tweet.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_tweet: str - The starting tweet from which to begin the thought process.\n",
    "    - model_name: str - The model to use for generating the tweets.\n",
    "    - num_iterations: int - The number of tweets to generate in sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tweets generated in sequence, each influenced by the previous tweet.\n",
    "    \"\"\"\n",
    "    tweet_text = initial_tweet\n",
    "    tweets = [tweet_text]  # Start with the initial tweet\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        prompt = f\"Let's start step by step to analyze this tweet: {tweet_text} Generate a similar tweet: \"\n",
    "        response = client.completions.create(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=1,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        tweet_text = response.choices[0].text.strip()  # Get the new tweet\n",
    "        tweets.append(tweet_text)  # Add the new tweet to the list\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Example usage:\n",
    "initial_tweet = \"Concerns about climate change are leading to new energy policies.\"\n",
    "generated_tweets = chain_of_thought_generation(initial_tweet)\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "llama = LlamaAPI(os.getenv('LLAMA_API'))\n",
    "\n",
    "# API Request JSON Cell\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a llama assistant that talks like a llama, starting every word with 'll'.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, happy llama day!\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming vs non streaming, look up and see whats going on.\n",
    "\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Extract the desired information from the following passage.:\\n\\nHi!\"},\n",
    "  ],\n",
    "  \"functions\": [\n",
    "        {'name': 'information_extraction',\n",
    "         'description': 'Extracts the relevant information from the passage.',\n",
    "         'parameters': {\n",
    "             'type': 'object',\n",
    "             'properties': {\n",
    "                 'sentiment': {\n",
    "                    'title': 'sentiment',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the sentiment encountered in the passage'\n",
    "                    },\n",
    "                 'aggressiveness': {\n",
    "                    'title': 'aggressiveness',\n",
    "                    'type': 'integer',\n",
    "                    'description': 'a 0-10 score of how aggressive the passage is'\n",
    "                    },\n",
    "                 'language': {\n",
    "                    'title': 'language',\n",
    "                    'type': 'string',\n",
    "                    'description': 'the language of the passage'\n",
    "                    }\n",
    "             },\n",
    "             'required': ['sentiment', 'aggressiveness', 'language']\n",
    "         }\n",
    "      }\n",
    "    ],\n",
    "  \"stream\": False,\n",
    "  \"function_call\": {\"name\": \"information_extraction\"},\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact details need to be known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is **Canberra**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "# Configure the API key\n",
    "# Configure the API key\n",
    "\n",
    "genai.configure(api_key= os.getenv(\"GEMINI_API\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "# Generate content with a simple text prompt\n",
    "response = model.generate_content([\"What is the capital of Australia?\"])\n",
    "\n",
    "# Print the response\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Art of API Design: Crafting Elegant and Powerful Interfaces\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "\n",
    "co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "response = co.chat(\n",
    "  model=\"command-r-plus\",\n",
    "  message=\"Write a title for a blog post about API design. Only output the title text.\"\n",
    ")\n",
    "\n",
    "print(response.text) # \"The Art of API Design: Crafting Elegant and Powerful Interfaces\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI21 Lab more model...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingApiKeyError",
     "evalue": "MissingApiKeyError API key must be supplied either globally in the ai21 namespace, or to be provided in the call args",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingApiKeyError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI21_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter your AI21 API key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the AI21 client with the API key\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mAI21Client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI21_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest_product_title\u001b[39m():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/ai21/clients/studio/ai21_client.py:49\u001b[0m, in \u001b[0;36mAI21Client.__init__\u001b[0;34m(self, api_key, api_host, headers, timeout_sec, num_retries, via, http_client, env_config, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     46\u001b[0m ):\n\u001b[1;32m     47\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m create_client_url(api_host \u001b[38;5;129;01mor\u001b[39;00m env_config\u001b[38;5;241m.\u001b[39mapi_host)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client \u001b[38;5;241m=\u001b[39m \u001b[43mAI21HTTPClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvia\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvia\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion \u001b[38;5;241m=\u001b[39m StudioCompletion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat: StudioChat \u001b[38;5;241m=\u001b[39m StudioChat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/ai21/ai21_http_client/ai21_http_client.py:23\u001b[0m, in \u001b[0;36mAI21HTTPClient.__init__\u001b[0;34m(self, api_key, requires_api_key, base_url, api_version, headers, timeout_sec, num_retries, via, http_client)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     http_client: Optional[HttpClient] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m ):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequires_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvia\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvia\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_headers(passed_headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_http_client(http_client\u001b[38;5;241m=\u001b[39mhttp_client, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/ai21/ai21_http_client/base_ai21_http_client.py:34\u001b[0m, in \u001b[0;36mBaseAI21HTTPClient.__init__\u001b[0;34m(self, api_key, requires_api_key, base_url, api_version, headers, timeout_sec, num_retries, via)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_api_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_key:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingApiKeyError()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url \u001b[38;5;241m=\u001b[39m base_url\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_version \u001b[38;5;241m=\u001b[39m api_version\n",
      "\u001b[0;31mMissingApiKeyError\u001b[0m: MissingApiKeyError API key must be supplied either globally in the ai21 namespace, or to be provided in the call args"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ai21 import AI21Client\n",
    "from ai21.models.chat import ChatMessage\n",
    "\n",
    "# Prompt the user to enter the API key if it's not set\n",
    "if \"AI21_API_KEY\" not in os.environ:\n",
    "    os.environ[\"AI21_API_KEY\"] = input(\"Please enter your AI21 API key: \")\n",
    "\n",
    "# Initialize the AI21 client with the API key\n",
    "client = AI21Client(os.getenv(\"AI21_API_KEY\"))\n",
    "\n",
    "def suggest_product_title():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"jamba-instruct-preview\",  # Latest model\n",
    "            messages=[ChatMessage(   # Single message with a single prompt\n",
    "                role=\"user\",\n",
    "                content=\"Write a product title for a sports T-shirt to be published on an online retail platform. Include the following keywords: activewear, gym, dryfit.\"\n",
    "            )],\n",
    "            temperature=0.8,\n",
    "            max_tokens=200  # You can also mention a max length in the prompt \"limit responses to twenty words\"\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to suggest a product title\n",
    "suggest_product_title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND_MODE: unix2003\n",
      "CONDA_DEFAULT_ENV: base\n",
      "CONDA_EXE: /opt/miniconda3/bin/conda\n",
      "CONDA_PREFIX: /opt/miniconda3\n",
      "CONDA_PROMPT_MODIFIER: (base) \n",
      "CONDA_PYTHON_EXE: /opt/miniconda3/bin/python\n",
      "CONDA_SHLVL: 1\n",
      "DISPLAY: /private/tmp/com.apple.launchd.vuHxpmElbG/org.xquartz:0\n",
      "HOME: /Users/jamiepan\n",
      "LOGNAME: jamiepan\n",
      "MallocNanoZone: 0\n",
      "OLDPWD: /\n",
      "ORIGINAL_XDG_CURRENT_DESKTOP: undefined\n",
      "PATH: /Users/jamiepan/.pyenv/versions/3.11.3/bin:/Users/jamiepan/.local/bin:/opt/miniconda3/bin:/opt/miniconda3/condabin:/Users/jamiepan/.pyenv/shims:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Library/Apple/usr/bin\n",
      "PWD: /\n",
      "PYENV_ROOT: /Users/jamiepan/.pyenv\n",
      "PYENV_SHELL: zsh\n",
      "SHELL: /bin/zsh\n",
      "SHLVL: 0\n",
      "SSH_AUTH_SOCK: /private/tmp/com.apple.launchd.4q3YQjiP3z/Listeners\n",
      "TMPDIR: /var/folders/86/0xt8cgcs6cs63qjy82mvg7hm0000gn/T/\n",
      "USER: jamiepan\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_CODE_CACHE_PATH: /Users/jamiepan/Library/Application Support/Code/CachedData/dc96b837cf6bb4af9cd736aa3af08cf8279f7685\n",
      "VSCODE_CRASH_REPORTER_PROCESS_TYPE: extensionHost\n",
      "VSCODE_CWD: /\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_IPC_HOOK: /Users/jamiepan/Library/Application Support/Code/1.89-main.sock\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en-gb\",\"osLocale\":\"en-au\",\"availableLanguages\":{},\"_languagePackSupport\":true}\n",
      "VSCODE_PID: 50469\n",
      "XPC_FLAGS: 0x0\n",
      "XPC_SERVICE_NAME: application.com.microsoft.VSCode.56426471.56426477\n",
      "_: /Users/jamiepan/Desktop/Visual Studio Code.app/Contents/MacOS/Electron\n",
      "__CFBundleIdentifier: com.microsoft.VSCode\n",
      "__CF_USER_TEXT_ENCODING: 0x1F5:0x0:0xF\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL: 1\n",
      "TWITTER_CONSUMER_KEY: Kb9FChkVxNgaiYgwGbVdXKaJ8\n",
      "TWITTER_CONSUMER_SECRET: l2pJl9VsOvCj6MT4Hr93g01Ad4JsO26xyPjv6zJLb8Gg6esUyv\n",
      "TWITTER_ACCESS_TOKEN: 1772073225801105408-tqFHOmYPqyls175uC377KsqX3t4lr3\n",
      "TWITTER_ACCESS_SECRET: l4iOqhhmsmoWH9lGlv5f7u2NFw1PaHCNrsSuLrWLOm2rz\n",
      "OPENAI_API_KEY: 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\n",
      "LLAMA_API: LL-DwhskGz0ai5WDqJfJ2yOzjlT8R1R0ntENPv2g66pVMPhtK6tvsIxKzZUkIj5ZNhG\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PYTHONUNBUFFERED: 1\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYTHON_FROZEN_MODULES: on\n",
      "LC_CTYPE: UTF-8\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "GEMINI: AIzaSyAMreykXDPh49daBj7yl2FTrmUEL61FNl4\n",
      "GEMINI_API: AIzaSyAMreykXDPh49daBj7yl2FTrmUEL61FNl4\n",
      "COHERE_API_KEY: S3pGYTHanvAYMRoup25RSP76Uozj01RIeD9fe40E\n",
      "AI21_API_KEY: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Print all environment variables to debug\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT experimentation with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.tokenization_bart because of the following error (look up to see its traceback):\nNo module named 'transformers.models.bart.tokenization_bart'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1535\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.bart.tokenization_bart'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartForConditionalGeneration, BartTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the BART model and tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1231\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1526\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1525\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1526\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1525\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1525\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1537\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1540\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.tokenization_bart because of the following error (look up to see its traceback):\nNo module named 'transformers.models.bart.tokenization_bart'"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = \"facebook/bart-large\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"Once upon a time, in a land far, far away, there was a brave knight named Arthur.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross output between models, see if certain model can figure out what prompts are asked by other model and check result\n",
    "\n",
    "Probelm: A lot of model requires paid money. Is it okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Fetch Tweet Text\n",
    "\n",
    "This function retrieves the text of a tweet by its ID using the Twitter API. It handles exceptions by printing an error message.\n",
    "\n",
    "\n",
    "Problem: Tweeter developer account need to be paid to use its recall API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(tweet_id):\n",
    "    \"\"\"Fetch the text of a tweet given its ID.\"\"\"\n",
    "    try:\n",
    "        tweet = twitter_api.get_status(tweet_id, tweet_mode='extended')\n",
    "        return tweet.full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweet {tweet_id}: {e}\")\n",
    "        return None\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
